{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from matplotlib import pyplot as plt #Use for image debugging\n",
    "import torch \n",
    "from torchvision import models, transforms #Need this to get VGG-11\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed batches (based on James' code)\n",
    "# The original data is organize slightly differently so it's somewhat messy. Sorry. \n",
    "\n",
    "def load_processed_batches(path, test = 0):\n",
    "    #Path is the directory where the files of interest are\n",
    "    \n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    print('Loading data...')\n",
    "    if path == 'cifar10_dataset/cifar-10-batches-py':\n",
    "        #The original data is in its own folder with some extra files\n",
    "        #So we only loop through the ones that we care about \n",
    "        files = os.listdir(path)\n",
    "        files2 = []\n",
    "        for i in range(len(files)): #If we want train data\n",
    "            if test == 0: \n",
    "                if 'data_batch' in files[i]: \n",
    "                    files2.append(files[i])\n",
    "            else: \n",
    "                if 'test_batch' in files[i]: \n",
    "                    files2.append(files[i])\n",
    "        for file in tqdm.tqdm(files2): #If we want test data\n",
    "            with open(os.path.join(path, file), 'rb') as f:\n",
    "                processed_batch_dict = pickle.load(f, encoding='bytes')\n",
    "                data.append(processed_batch_dict[b'data'])\n",
    "                labels.append(processed_batch_dict[b'labels'])\n",
    "        \n",
    "        #Store the data and labels \n",
    "        data = np.concatenate(data)\n",
    "        data = data.astype(np.float32) / 255 #Divide by 255 to get into 0 to 1 range \n",
    "        labels = np.concatenate(labels)\n",
    "        \n",
    "        #Reshape to the same dimensions as the processed data\n",
    "        data = data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "\n",
    "    else: #If we're using black boxes or Gaussian noise data \n",
    "        if test == 1: path = path + '_test' \n",
    "        files = os.listdir(path)\n",
    "        for file in tqdm.tqdm(files):\n",
    "            with open(os.path.join(path, file), 'rb') as f:\n",
    "                processed_batch_dict = pickle.load(f, encoding='bytes')\n",
    "                data.append(processed_batch_dict['data'])\n",
    "                labels.append(processed_batch_dict['labels'])\n",
    "        #Store the data and labels \n",
    "        data = np.concatenate(data)\n",
    "        data = data.astype(np.float32) / 255 #Divide by 255 to get into 0 to 1 range \n",
    "\n",
    "        labels = np.concatenate(labels)\n",
    "        labels = np.repeat(labels,9) #assume that the same image is repeated 9 times for each of the superpixels (3-by-3)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "def get_data(ds, normalize, test = 0): \n",
    "\n",
    "    #ds = dataset \n",
    "    #ds = 0 for standard CIFAR\n",
    "    #ds = 1 for black box\n",
    "    #ds = 2 for Gaussian noise \n",
    "\n",
    "    if ds == 0: \n",
    "        path = 'cifar10_dataset/cifar-10-batches-py'\n",
    "    elif ds == 1: \n",
    "        path = 'processed_batches_boxes' \n",
    "    elif ds == 2: \n",
    "        path = 'processed_batches_gaussian_noise'\n",
    "\n",
    "    batch_size = 64\n",
    "    num_workers = 2 \n",
    "\n",
    "    train_loader = []; \n",
    "    val_loader = []; \n",
    "    test_loader = []; \n",
    "\n",
    "    if test == 0: \n",
    "        processed_images, processed_labels = load_processed_batches(path,test=test)\n",
    "        processed_images = processed_images.transpose(0,3,1,2) #Get into the appropriate shape for training\n",
    "\n",
    "        #Train and validation data split \n",
    "        trainData, valData, trainLabel, valLabel = train_test_split(processed_images, processed_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "        #Transform data\n",
    "        print('Preparing data for loading...')\n",
    "        train_set = torch.utils.data.TensorDataset(normalize(torch.tensor(trainData)),torch.tensor(trainLabel).type(torch.LongTensor))\n",
    "        val_set = torch.utils.data.TensorDataset(normalize(torch.tensor(valData)),torch.tensor(valLabel).type(torch.LongTensor)) \n",
    "\n",
    "        #Set-up dataloaders\n",
    "        train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers = num_workers)\n",
    "        val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers = num_workers)\n",
    "    \n",
    "    else: #Get data and format it for testing \n",
    "        processed_images, processed_labels = load_processed_batches(path,test=test)\n",
    "        processed_images = processed_images.transpose(0,3,1,2) #Get into the appropriate shape for training\n",
    "        \n",
    "        print('Preparing data for loading...')\n",
    "        test_set = torch.utils.data.TensorDataset(normalize(torch.tensor(processed_images)),torch.tensor(processed_labels).type(torch.LongTensor))\n",
    "        test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers = num_workers)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def train_network(train_loader, device, num_epochs = 2, momentum = 0.9, learning_rate = 0.001, weight_decay = 0.001): \n",
    "\n",
    "    #Get model\n",
    "    mod = models.vgg11(weights=None)\n",
    "    mod.classifier[6].out_features = 10 #Adjust final layer to have the right number of classes \n",
    "\n",
    "    #Move model to GPU (if available) \n",
    "    mod.to(device)\n",
    "    \n",
    "    #Training \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(mod.parameters(), lr=learning_rate, weight_decay=weight_decay, momentum=momentum)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):  \n",
    "            # Move tensors to the configured device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = mod(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if num_epochs > 3: \n",
    "            print ('Epoch [{}/{}], Loss: {:.4f}' \n",
    "                        .format(epoch+1, num_epochs, i+1, loss.item()))\n",
    "        \n",
    "    return mod \n",
    "\n",
    "def test_model(test_loader, mod, device): \n",
    "    # For test data or validation data\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = mod(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            del images, labels, outputs\n",
    "\n",
    "        accuracy = 100 * correct / total; \n",
    "\n",
    "        #print('Accuracy of the network on validation images: {} %'.format(accuracy)) \n",
    "\n",
    "    return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 40.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: [0.32768 0.32768 0.32768]\n",
      "Std Dev: [0.27755317 0.26929596 0.26811677]\n",
      "NVIDIA GeForce RTX 4090\n",
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 40.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for loading...\n",
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 36.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for loading...\n",
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 32.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for loading...\n"
     ]
    }
   ],
   "source": [
    "### Set up constants for experiments ### \n",
    "\n",
    "# Normalize images via the statistics of the original dataset\n",
    "path = 'cifar10_dataset/cifar-10-batches-py'\n",
    "processed_images, _ = load_processed_batches(path,test=0)\n",
    "\n",
    "imMean = np.mean(processed_images.reshape(-1,3),axis=0)\n",
    "imStd = np.std(processed_images.reshape(-1,3),axis=0)\n",
    "\n",
    "print('Mean:',imMean)\n",
    "print('Std Dev:', imStd)\n",
    "\n",
    "#Set-up normalization\n",
    "normalize = transforms.Normalize(mean=imMean,std=imStd)\n",
    "\n",
    "#Get device \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if torch.cuda.is_available(): \n",
    "    print(torch.cuda.get_device_name(0))\n",
    "\n",
    "#Get test data \n",
    "_, _, cifar_test = get_data(0, normalize, test = 1) #this one is saved as 1 batch, the other two are in 5\n",
    "_, _, black_test = get_data(1, normalize, test = 1)\n",
    "_, _, gauss_test = get_data(2, normalize, test = 1)\n",
    "\n",
    "#Other parameters: \n",
    "momentums = np.linspace(0.5,1.3,5)\n",
    "lrs = np.logspace(-1,-5,5)\n",
    "decays = np.logspace(-1,-5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 52.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for loading...\n",
      "Optimizing momentum...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:10<00:00, 26.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing learning rate...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:10<00:00, 26.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing weight decay...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:10<00:00, 26.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 625.0000\n",
      "Epoch [2/20], Loss: 625.0000\n",
      "Epoch [3/20], Loss: 625.0000\n",
      "Epoch [4/20], Loss: 625.0000\n",
      "Epoch [5/20], Loss: 625.0000\n",
      "Epoch [6/20], Loss: 625.0000\n",
      "Epoch [7/20], Loss: 625.0000\n",
      "Epoch [8/20], Loss: 625.0000\n",
      "Epoch [9/20], Loss: 625.0000\n",
      "Epoch [10/20], Loss: 625.0000\n",
      "Epoch [11/20], Loss: 625.0000\n",
      "Epoch [12/20], Loss: 625.0000\n",
      "Epoch [13/20], Loss: 625.0000\n",
      "Epoch [14/20], Loss: 625.0000\n",
      "Epoch [15/20], Loss: 625.0000\n",
      "Epoch [16/20], Loss: 625.0000\n",
      "Epoch [17/20], Loss: 625.0000\n",
      "Epoch [18/20], Loss: 625.0000\n",
      "Epoch [19/20], Loss: 625.0000\n",
      "Epoch [20/20], Loss: 625.0000\n",
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  7.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for loading...\n",
      "Optimizing momentum...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [12:06<00:00, 145.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing learning rate...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [11:57<00:00, 143.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing weight decay...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [11:52<00:00, 142.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 5625.0000\n",
      "Epoch [2/20], Loss: 5625.0000\n",
      "Epoch [3/20], Loss: 5625.0000\n",
      "Epoch [4/20], Loss: 5625.0000\n",
      "Epoch [5/20], Loss: 5625.0000\n",
      "Epoch [6/20], Loss: 5625.0000\n",
      "Epoch [7/20], Loss: 5625.0000\n",
      "Epoch [8/20], Loss: 5625.0000\n",
      "Epoch [9/20], Loss: 5625.0000\n",
      "Epoch [10/20], Loss: 5625.0000\n",
      "Epoch [11/20], Loss: 5625.0000\n",
      "Epoch [12/20], Loss: 5625.0000\n",
      "Epoch [13/20], Loss: 5625.0000\n",
      "Epoch [14/20], Loss: 5625.0000\n",
      "Epoch [15/20], Loss: 5625.0000\n",
      "Epoch [16/20], Loss: 5625.0000\n",
      "Epoch [17/20], Loss: 5625.0000\n",
      "Epoch [18/20], Loss: 5625.0000\n",
      "Epoch [19/20], Loss: 5625.0000\n",
      "Epoch [20/20], Loss: 5625.0000\n",
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  7.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for loading...\n",
      "Optimizing momentum...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [11:54<00:00, 142.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing learning rate...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [11:53<00:00, 142.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing weight decay...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [11:53<00:00, 142.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 5625.0000\n",
      "Epoch [2/20], Loss: 5625.0000\n",
      "Epoch [3/20], Loss: 5625.0000\n",
      "Epoch [4/20], Loss: 5625.0000\n",
      "Epoch [5/20], Loss: 5625.0000\n",
      "Epoch [6/20], Loss: 5625.0000\n",
      "Epoch [7/20], Loss: 5625.0000\n",
      "Epoch [8/20], Loss: 5625.0000\n",
      "Epoch [9/20], Loss: 5625.0000\n",
      "Epoch [10/20], Loss: 5625.0000\n",
      "Epoch [11/20], Loss: 5625.0000\n",
      "Epoch [12/20], Loss: 5625.0000\n",
      "Epoch [13/20], Loss: 5625.0000\n",
      "Epoch [14/20], Loss: 5625.0000\n",
      "Epoch [15/20], Loss: 5625.0000\n",
      "Epoch [16/20], Loss: 5625.0000\n",
      "Epoch [17/20], Loss: 5625.0000\n",
      "Epoch [18/20], Loss: 5625.0000\n",
      "Epoch [19/20], Loss: 5625.0000\n",
      "Epoch [20/20], Loss: 5625.0000\n"
     ]
    }
   ],
   "source": [
    "# for each dataset \n",
    "#   for each momentum \n",
    "#   for each learning rate \n",
    "#   for each weight decay \n",
    "#   train for 20 epochs (save network)\n",
    "#   evalulate on all three test sets\n",
    "\n",
    "#Save the hyperparamters: \n",
    "bestHP = np.zeros((3,3))\n",
    "\n",
    "#Hyperparameter storage \n",
    "#               cifar   black   gauss \n",
    "#momentum       \n",
    "#learning rate \n",
    "#weight decay \n",
    "\n",
    "#Save the performances \n",
    "testMatrix = np.zeros((3,3))\n",
    "#               cifar   black   gauss \n",
    "#cifar_test       \n",
    "#black_test\n",
    "#gauss_test  \n",
    "\n",
    "for ds in np.arange(3): \n",
    "    \n",
    "    train_loader, val_loader, _ = get_data(ds, normalize, test = 0)\n",
    "\n",
    "    ## Momentum ##\n",
    "    acc_list = np.zeros(5) #Initalize validation accuracy matrix \n",
    "    print('Optimizing momentum...')\n",
    "    for i in tqdm.tqdm(np.arange(len(momentums))): \n",
    "        mod = train_network(train_loader, device, num_epochs = 2, momentum = momentums[i], learning_rate = 0.001, weight_decay = 0.001)\n",
    "        acc_list[i] = test_model(val_loader, mod, device)\n",
    "    bestIdx =  np.argmax(acc_list) #Get the index of the best value \n",
    "    bestHP[0,ds] = momentums[bestIdx] #Save the value to the hyperparamter matrix \n",
    "\n",
    "    ## Learning rate ##\n",
    "    print('Optimizing learning rate...')\n",
    "    acc_list = np.zeros(5) #Initalize validation accuracy matrix \n",
    "    for i in tqdm.tqdm(np.arange(len(lrs))): \n",
    "        mod = train_network(train_loader, device, num_epochs = 2, momentum = bestHP[0,ds], learning_rate = lrs[i], weight_decay = 0.001)\n",
    "        acc_list[i] = test_model(val_loader, mod, device)\n",
    "    bestIdx =  np.argmax(acc_list) #Get the index of the best value \n",
    "    bestHP[1,ds] = lrs[bestIdx] #Save the value to the hyperparamter matrix \n",
    "\n",
    "    ## Weight decay ##\n",
    "    print('Optimizing weight decay...')\n",
    "    acc_list = np.zeros(5) #Initalize validation accuracy matrix \n",
    "    for i in tqdm.tqdm(np.arange(len(decays))): \n",
    "        mod = train_network(train_loader, device, num_epochs = 2, momentum = bestHP[0,ds], learning_rate = bestHP[1,ds], weight_decay = decays[i])\n",
    "        acc_list[i] = test_model(val_loader, mod, device)\n",
    "    bestIdx =  np.argmax(acc_list) #Get the index of the best value \n",
    "    bestHP[2,ds] = decays[bestIdx] #Save the value to the hyperparamter matrix \n",
    "\n",
    "    #Train the model for many epochs with the best hyperparameters\n",
    "    mod = train_network(train_loader, device, num_epochs = 20, momentum = bestHP[0,ds], learning_rate = bestHP[1,ds], weight_decay = bestHP[2,ds])\n",
    "    \n",
    "    #Test the model on each test set \n",
    "    testMatrix[0,ds] = test_model(cifar_test, mod, device)\n",
    "    testMatrix[1,ds] = test_model(black_test, mod, device)\n",
    "    testMatrix[2,ds] = test_model(gauss_test, mod, device)\n",
    "\n",
    "    #Save the model (just in case) \n",
    "    if ds == 0: \n",
    "        mName = 'cifar_model'\n",
    "    elif ds == 1: \n",
    "        mName = 'black_model'\n",
    "    elif ds == 2: \n",
    "        mName = 'gauss_model'\n",
    "    \n",
    "    torch.save(mod.state_dict(),mName)\n",
    "\n",
    "    #Note: \n",
    "    # To load these models again do: \n",
    "    # mod = models.vgg11(weights=None) \n",
    "    # mod.load_state_dict(torch.load('insert_file_name_here'))\n",
    "\n",
    "#Save the results\n",
    "np.save('optimized hyperparameters.npy',bestHP)\n",
    "np.save('test_accuracies.npy',testMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIFAR model</th>\n",
       "      <th>Black model</th>\n",
       "      <th>Gauss model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Momentum</th>\n",
       "      <td>0.90000</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.9000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Learning rate</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weight decay</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               CIFAR model  Black model  Gauss model\n",
       "Momentum           0.90000       0.9000       0.9000\n",
       "Learning rate      0.00100       0.0100       0.0100\n",
       "Weight decay       0.00001       0.0001       0.0001"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colLabel = [\"CIFAR model\", \"Black model\", \"Gauss model\"]\n",
    "rowLabel1 = [\"Momentum\", \"Learning rate\", \"Weight decay\"]\n",
    "rowLabel2 = [\"CIFAR test\", \"Black test\", \"Gauss test\"]\n",
    "\n",
    "pd.DataFrame(bestHP,rowLabel1,colLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIFAR model</th>\n",
       "      <th>Black model</th>\n",
       "      <th>Gauss model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CIFAR test</th>\n",
       "      <td>73.900000</td>\n",
       "      <td>79.870000</td>\n",
       "      <td>78.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Black test</th>\n",
       "      <td>55.713333</td>\n",
       "      <td>77.111111</td>\n",
       "      <td>49.623333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gauss test</th>\n",
       "      <td>72.440000</td>\n",
       "      <td>77.758889</td>\n",
       "      <td>77.797778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            CIFAR model  Black model  Gauss model\n",
       "CIFAR test    73.900000    79.870000    78.200000\n",
       "Black test    55.713333    77.111111    49.623333\n",
       "Gauss test    72.440000    77.758889    77.797778"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(testMatrix,rowLabel2,colLabel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
